{
    "timelineData": [
        {
            "year": 1957,
            "title": "Creation of the First Computer Programming Language",
            "summary": "In 1957, IBM released FORTRAN, the first widely adopted high-level programming language. Designed primarily for scientists and engineers, FORTRAN simplified the process of instructing computers, replacing complicated machine code with more human-readable syntax. This shift was revolutionary, as it allowed programmers to focus on problem-solving rather than memorizing binary commands.",
            "image": "https://m.media-amazon.com/images/I/61G7cDe5VmL._SL1431_.jpg",
            "fullContent": "<p>In 1957, IBM released <strong>FORTRAN</strong> (Formula Translation), the first widely adopted high-level programming language. Designed primarily for scientists and engineers, FORTRAN simplified the process of instructing computers, replacing complicated machine code with more human-readable syntax. This shift was revolutionary, as it allowed programmers to focus on problem-solving rather than memorizing binary commands.</p><p>FORTRAN's structured approach to programming made complex calculations faster and more reliable. It set the stage for modern programming languages by introducing concepts like loops, conditional statements, and modular code. The language was so influential that it continued to evolve for decades, with later versions still in use today for scientific computing.</p><p>The introduction of FORTRAN marked a turning point in computing history. It opened programming to a broader audience beyond mathematicians and computer scientists, enabling innovations in engineering, physics, and space exploration. By making computers more accessible and efficient, FORTRAN laid the groundwork for the software industry and inspired the creation of countless other languages. This milestone proved that computers could be programmed in ways that humans could easily understand — a concept that remains at the heart of modern software development.</p>"
        },
        {
            "year": 1975,
            "title": "Introduction of the First Personal Computer",
            "summary": "In 1975, the launch of the Altair 8800 marked the beginning of the personal computer revolution. Sold as a kit through Popular Electronics magazine, the Altair was not the sleek, ready-to-use device we know today. It was a box of switches and lights, but it captured the imagination of tech enthusiasts worldwide",
            "image": "https://adwaterandstir.com/wp-content/uploads/2017/08/image-844x633.jpg",
            "fullContent": "<p>In 1975, the launch of the <strong>Altair 8800</strong> marked the beginning of the personal computer revolution. Sold as a kit through Popular Electronics magazine, the Altair was not the sleek, ready-to-use device we know today. It was a box of switches and lights, but it captured the imagination of tech enthusiasts worldwide.</p><p>The Altair's success sparked the formation of key companies, including Microsoft, which created a version of BASIC specifically for it. This moment was a turning point because computing power was no longer confined to corporations, universities, or government labs. Hobbyists could now own and program their own machines, laying the foundation for the home computing industry.</p><p>Although primitive by modern standards, the Altair's significance lies in accessibility. It inspired innovation, fueled a growing tech culture, and motivated inventors to make computers user-friendly for the general public. This wave eventually led to iconic machines like the Apple II and IBM PC. The Altair's introduction represented more than just a product launch — it was the start of a cultural shift that turned computers from niche tools into everyday essentials.</p>"
        },
        {
            "year": 1983,
            "title": "Invention of the Internet",
            "summary": "In 1983, the modern Internet was born when ARPANET officially adopted the TCP/IP protocol. This standardized communication method allowed different computer networks to connect and exchange information seamlessly, creating a unified global system. Prior to TCP/IP, networks used incompatible protocols, making large-scale communication difficult.",
            "image": "https://media.geeksforgeeks.org/wp-content/uploads/20241107024252407375/History-of-Internet.webp",
            "fullContent": "<p>In 1983, the modern <strong>Internet</strong> was born when ARPANET officially adopted the <strong>TCP/IP protocol</strong>. This standardized communication method allowed different computer networks to connect and exchange information seamlessly, creating a unified global system. Prior to TCP/IP, networks used incompatible protocols, making large-scale communication difficult.</p><p>The adoption of TCP/IP transformed isolated networks into an interconnected \"network of networks.\" This new infrastructure made it possible for researchers, government agencies, and eventually the public to share data instantly across the globe. While email and file sharing were early uses, the framework also enabled future innovations such as online commerce, streaming, and social media.</p><p>The invention of the Internet was not just a technical breakthrough but also a social one. It laid the groundwork for a level of connectivity that would reshape education, business, and communication. By enabling real-time global interaction, it has bridged geographical distances, accelerated information exchange, and become one of the most influential creations in human history.</p>"
        },
        {
            "year": 1991,
            "title": "Development of the World Wide Web",
            "summary": "In 1991, Tim Berners-Lee introduced the World Wide Web, fundamentally changing how people accessed and shared information. Unlike the Internet itself, which already existed, the Web provided a user-friendly interface using web pages, hyperlinks, and browsers. This made navigating vast amounts of online information far more accessible.",
            "image": "https://cds.cern.ch/images/CERN-GE-9407011-31/file?size=large",
            "fullContent": "<p>In 1991, <strong>Tim Berners-Lee</strong> introduced the <strong>World Wide Web</strong>, fundamentally changing how people accessed and shared information. Unlike the Internet itself, which already existed, the Web provided a user-friendly interface using web pages, hyperlinks, and browsers. This made navigating vast amounts of online information far more accessible.</p><p>Berners-Lee's creation included three key technologies: HTML (to create web pages), HTTP (to transfer data), and URLs (to locate resources). The first-ever website was a simple page explaining how the Web worked, but it sparked a wave of creativity and innovation. Suddenly, individuals and organizations could publish content for a global audience without specialized equipment.</p><p>The World Wide Web transformed the Internet from a technical network into a platform for communication, commerce, entertainment, and knowledge sharing. It paved the way for online news, e-commerce giants, social networking, and virtually every digital service we rely on today. By democratizing information access, it empowered billions of people, shaping a truly interconnected world.</p>"
        },
        {
            "year": 1993,
            "title": "Introduction of the First Search Engine",
            "summary": "In 1993, JumpStation emerged as the first web search engine, revolutionizing the way people found information online. Before search engines, locating a specific web page required knowing its exact address or stumbling upon it through links — a frustrating and time-consuming process.",
            "image": "https://upload.wikimedia.org/wikipedia/en/3/3d/Jumpstation.png",
            "fullContent": "<p>In 1993, <strong>JumpStation</strong> emerged as the first web search engine, revolutionizing the way people found information online. Before search engines, locating a specific web page required knowing its exact address or stumbling upon it through links — a frustrating and time-consuming process.</p><p>JumpStation automated this by using web crawlers to index page titles and headers, allowing users to search and retrieve relevant sites quickly. Although primitive compared to modern engines like Google, it laid the foundation for search-based navigation of the Web. This made the Internet vastly more usable and helped it grow into an essential daily resource.</p><p>The introduction of search engines marked a major shift in how humans interacted with technology. Information retrieval became faster, more efficient, and less dependent on personal knowledge of the Web's structure. This innovation not only fueled the expansion of online content but also shaped the entire digital economy, as visibility in search results soon became a powerful tool for businesses and creators alike.</p>"
        },
        {
            "year": 2004,
            "title": "Rise of Social Media Platforms",
            "summary": "In 2004, the launch of Facebook marked a major turning point in the history of online communication. While earlier platforms like Friendster and MySpace existed, Facebook’s cleaner design, real-name policy, and network-based expansion made it a game changer. It began as a college networking site but quickly grew to include users worldwide.",
            "image": "https://media.webdesignerdepot.com/spai/q_lossy+ret_img+to_auto/webdesignerdepot-wp.s3.us-east-2.amazonaws.com/2023/07/08101955/featured_social_media_logos.png",
            "fullContent": "<p>In 2004, the launch of <strong>Facebook</strong> marked a major turning point in the history of online communication. While earlier platforms like Friendster and MySpace existed, Facebook's cleaner design, real-name policy, and network-based expansion made it a game changer. It began as a college networking site but quickly grew to include users worldwide.</p><p>Social media transformed how people connected, shared ideas, and consumed news. It gave individuals a global voice, enabling grassroots movements, viral trends, and real-time updates from anywhere in the world. Businesses also tapped into this new space, using targeted advertising to reach audiences with unprecedented precision.</p><p>While the rise of social media has brought challenges like misinformation and privacy concerns, it has undeniably reshaped culture, politics, and personal relationships. From activism to entertainment, social platforms have become digital town squares — places where the world meets, talks, and organizes.</p>"
        },
        {
            "year": 2006,
            "title": "Evolution of Cloud Computing",
            "summary": "In 2006, Amazon Web Services (AWS) launched EC2, making scalable cloud computing available to the masses. This meant businesses could rent virtual servers on demand instead of buying costly hardware. It revolutionized how companies deployed applications, enabling flexibility, speed, and cost savings.",
            "image": "https://res.cloudinary.com/dthpnue1d/image/upload/v1745560485/A_Beginner_s_Guide_Types_of_Cloud_Computing_Services_to_Integrate_in_Business_ab843f1162.webp",
            "fullContent": "<p>In 2006, <strong>Amazon Web Services (AWS)</strong> launched <strong>EC2</strong>, making scalable cloud computing available to the masses. This meant businesses could rent virtual servers on demand instead of buying costly hardware. It revolutionized how companies deployed applications, enabling flexibility, speed, and cost savings.</p><p>Cloud computing shifted the technology model from ownership to subscription. Startups could now launch with minimal infrastructure investment, while large enterprises could scale operations instantly to meet demand. This democratization of computing power opened the door for innovations in AI, data analytics, and global collaboration.</p><p>Today, cloud platforms run critical services, from streaming media to online banking. The model pioneered in 2006 has become the backbone of the digital economy, enabling global-scale systems that would have been unimaginable just two decades earlier.</p>"
        },
        {
            "year": 2007,
            "title": "Launch of the First Smartphone",
            "summary": "In 2007, Apple introduced the iPhone, blending a phone, music player, and Internet communicator into a single sleek device. It replaced physical keyboards with a responsive touchscreen, introducing a new era of mobile interaction. The App Store, launched a year later, opened the door for third-party developers to create millions of apps that would redefine daily life.",
            "image": "https://i.guim.co.uk/img/media/5d1906e32977e7cd61d8a13a15895839390d06da/0_89_1373_824/master/1373.jpg?width=1200&quality=85&auto=format&fit=max&s=22aa3bf1ba9b668e1704afc6e1096090",
            "fullContent": "<p>In 2007, Apple introduced the <strong>iPhone</strong>, blending a phone, music player, and Internet communicator into a single sleek device. It replaced physical keyboards with a responsive touchscreen, introducing a new era of mobile interaction. The App Store, launched a year later, opened the door for third-party developers to create millions of apps that would redefine daily life.</p><p>The smartphone became more than a communication tool — it evolved into a personal assistant, camera, navigation device, and entertainment hub. With mobile Internet access, people could connect, work, and play anywhere, transforming both productivity and leisure.</p><p>The iPhone's influence extended beyond technology to culture and economics. It sparked an industry-wide shift toward mobile-first design and gave rise to gig economy apps, social media platforms, and instant access to information. Today, smartphones are indispensable, shaping how billions of people interact with the world.</p>"
        },
        {
            "year": 2016,
            "title": "Advancement of Virtual Reality Technology",
            "summary": "In 2016, the launch of the Oculus Rift and HTC Vive brought high-quality virtual reality (VR) to consumers. These devices offered immersive 3D environments where users could interact with digital worlds in real time.",
            "image": "https://unity.com/_next/image?url=https%3A%2F%2Fcdn.bfldr.com%2FS5BC9Y64%2Fat%2Fstjbxwvg6z2rp5t4sz2kcf4h%2FUnity_Stock_760842796.jpg%3Fauto%3Dwebp&w=3840&q=75",
            "fullContent": "<p>In 2016, the launch of the <strong>Oculus Rift</strong> and <strong>HTC Vive</strong> brought high-quality <strong>virtual reality (VR)</strong> to consumers. These devices offered immersive 3D environments where users could interact with digital worlds in real time.</p><p>VR technology had existed for decades, but earlier versions were bulky, expensive, and limited. The 2016 breakthroughs combined precise motion tracking, high-resolution displays, and powerful gaming hardware to deliver compelling, interactive experiences. Industries beyond gaming — including healthcare, education, and design — began experimenting with VR for training, therapy, and creative projects.</p><p>While adoption is still growing, VR represents a new frontier in human-computer interaction. It blurs the boundaries between the digital and physical, opening possibilities for remote collaboration, simulated exploration, and entertainment experiences beyond the limits of traditional screens.</p>"
        },
        {
            "year": 2020,
            "title": "The AI Boom: How the 2020s Redefined Intelligence",
            "summary": "The 2020s marked the decade when artificial intelligence evolved from a promising technology to a fundamental force reshaping industries, creativity, and human-machine collaboration.",
            "image": "https://velvetshark.com/_next/image?url=%2Fimages%2Fai-company-logos-that-look-like-buttholes%2Fwhy-do-AI-company-logos-look-like-buttholes.png&w=1920&q=75",
            "fullContent": "<p>The 2020s witnessed artificial intelligence's dramatic evolution from specialized tool to ubiquitous force. The pivotal moment came with <strong>GPT-3</strong>'s 2020 debut, demonstrating unprecedented language mastery that reshaped expectations. Subsequent years saw AI conquer creative domains, generating artworks indistinguishable from human creations and composing original music. By mid-decade, these systems were diagnosing illnesses with superhuman accuracy and accelerating scientific breakthroughs in fields from medicine to climate science.</p><p>This transformation wasn't without growing pains. Society struggled to adapt as AI disrupted traditional employment patterns while creating new opportunities. Ethical dilemmas emerged around machine-generated content authenticity and the appropriate boundaries of automation. The technology's rapid advancement outpaced regulatory frameworks, creating tensions between innovation and oversight.</p><p>As we approach 2025, AI has become the invisible infrastructure powering daily life. It personalizes education, optimizes city management, and even influences artistic expression. The most profound impact may be how it's redefining human potential, allowing us to focus on creative and strategic thinking while machines handle routine cognitive tasks. This decade has set the stage for an even more transformative future where artificial and human intelligence increasingly collaborate.</p>"
        }   
    ]
}